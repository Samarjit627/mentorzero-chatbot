import os
import subprocess
import datetime
import json
import csv

# Define paths to scraping scripts and data directories
SCRAPING_SCRIPTS = {
    "paul_graham": "scrape_paul_graham.py",
    "yc_blog": "scrape_yc_blog.py",
    "yc_partner_interviews": "scrape_yc_partner_interviews.py",
    "book_summaries": "scrape_book_summaries.py",
    "a16z": "scrape_a16z.py",
    "elad_gil": "scrape_elad_gil.py",
    "tren_griffin": "scrape_tren_griffin.py",
    "yc_ai": "scrape_yc_ai.py",
    # "startup_failures": "scrape_startup_failures.py", # Temporarily disabled due to persistent 403 errors
    # "startup_school": "scrape_startup_school.py", # Temporarily disabled due to scraping issues
    # "yc_podcast": "scrape_yc_podcast.py", # Temporarily disabled due to 403 errors
}

DATA_DIRECTORIES = {
    "paul_graham": "paul_graham_essays",
    "yc_blog": "yc_blog",
    "yc_partner_interviews": "investor_thinking",
    "book_summaries": "book_summaries",
    "a16z": "investor_thinking",
    "elad_gil": "investor_thinking",
    "tren_griffin": "investor_thinking",
    "yc_ai": "ai_startup_playbook",
    # "startup_failures": "startup_failures",
    # "startup_school": "startup_school",
    # "yc_podcast": "yc_podcast",
}

KNOWLEDGE_BASE_SCRIPT = "create_knowledge_base.py"

# --- Transparency CSV Configuration ---
CSV_FILE = "yc_video_ingestion_report.csv"
CSV_HEADERS = [
    "Video_ID", "Video_URL", "Video_Title", "Video_Published_Date",
    "Video_Duration_Seconds", "Transcript_Tokens", "Transcript_Extracted_YN",
    "Ingestion_Status", "Last_Ingestion_Date", "Notes"
]

def run_scraping_scripts():
    print("\n--- Running Scraping Scripts ---")
    results = {}
    for source, script_name in SCRAPING_SCRIPTS.items():
        print(f"Running {script_name} for {source}...")
        try:
            process = subprocess.run(
                ["python3", script_name],
                capture_output=True,
                text=True,
                check=True,
                cwd="/home/ubuntu"
            )
            print(f"  {source} scraping successful.")
            results[source] = {"status": "Success", "output": process.stdout}
        except subprocess.CalledProcessError as e:
            print(f"  {source} scraping FAILED: {e}")
            results[source] = {"status": "Failed", "error": e.stderr}
        except FileNotFoundError:
            print(f"  {script_name} not found. Skipping {source}.")
            results[source] = {"status": "Skipped", "error": "Script not found"}
    return results

def run_knowledge_base_creation():
    print("\n--- Creating/Updating Knowledge Base ---")
    try:
        process = subprocess.run(
            ["python3", KNOWLEDGE_BASE_SCRIPT],
            capture_output=True,
            text=True,
            check=True,
            cwd="/home/ubuntu"
        )
        print("  Knowledge base creation successful.")
        return {"status": "Success", "output": process.stdout}
    except subprocess.CalledProcessError as e:
        print(f"  Knowledge base creation FAILED: {e}")
        return {"status": "Failed", "error": e.stderr}
    except FileNotFoundError:
        print(f"  {KNOWLEDGE_BASE_SCRIPT} not found. Skipping knowledge base update.")
        return {"status": "Skipped", "error": "Script not found"}

def generate_ingestion_report():
    print("\n--- Generating Ingestion Report ---")
    report_data = []
    current_date = datetime.date.today().isoformat()

    # This is a placeholder. In a real scenario, you would parse the output of scraping
    # scripts and the knowledge base creation to populate this more accurately.
    # For now, it will reflect the status of the scraping scripts.
    
    # Example: Populate with dummy data or actual scraped metadata if available
    # For a full implementation, this would require parsing the JSON files generated by scrapers
    # and potentially querying the FAISS metadata for token counts and extraction status.
    
    # Placeholder for YouTube content (since direct scraping is problematic)
    # This would ideally come from yt-dlp metadata
    report_data.append({
        "Video_ID": "N/A",
        "Video_URL": "https://www.youtube.com/ycombinator",
        "Video_Title": "YC YouTube Channel (Placeholder)",
        "Video_Published_Date": "N/A",
        "Video_Duration_Seconds": "N/A",
        "Transcript_Tokens": "N/A",
        "Transcript_Extracted_YN": "No",
        "Ingestion_Status": "Partial/Manual Check Needed",
        "Last_Ingestion_Date": current_date,
        "Notes": "Direct YouTube scraping is challenging. Manual verification needed."
    })

    # Add data from successfully scraped sources
    for source, data_dir in DATA_DIRECTORIES.items():
        json_file_path = os.path.join(data_dir, f"{source}_articles.json") # Assuming this naming convention
        if os.path.exists(json_file_path):
            try:
                with open(json_file_path, "r", encoding="utf-8") as f:
                    articles = json.load(f)
                for article in articles:
                    report_data.append({
                        "Video_ID": "N/A", # Not applicable for non-video content
                        "Video_URL": article.get("url", "N/A"),
                        "Video_Title": article.get("title", "N/A"),
                        "Video_Published_Date": "N/A", # Not always available for articles
                        "Video_Duration_Seconds": "N/A",
                        "Transcript_Tokens": len(article.get("content", "").split()) if article.get("content") else 0,
                        "Transcript_Extracted_YN": "Yes" if article.get("content") else "No",
                        "Ingestion_Status": "Success",
                        "Last_Ingestion_Date": current_date,
                        "Notes": f"Content from {source}"
                    })
            except Exception as e:
                print(f"Error reading {json_file_path}: {e}")
                report_data.append({
                    "Video_ID": "N/A", "Video_URL": "N/A", "Video_Title": f"Error loading {source}",
                    "Video_Published_Date": "N/A", "Video_Duration_Seconds": "N/A",
                    "Transcript_Tokens": "N/A", "Transcript_Extracted_YN": "No",
                    "Ingestion_Status": "Failed", "Last_Ingestion_Date": current_date,
                    "Notes": str(e)
                })
        else:
            report_data.append({
                "Video_ID": "N/A", "Video_URL": "N/A", "Video_Title": f"No data file for {source}",
                "Video_Published_Date": "N/A", "Video_Duration_Seconds": "N/A",
                "Transcript_Tokens": "N/A", "Transcript_Extracted_YN": "No",
                "Ingestion_Status": "Skipped/Missing", "Last_Ingestion_Date": current_date,
                "Notes": f"Expected file {json_file_path} not found."
            })

    with open(CSV_FILE, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=CSV_HEADERS)
        writer.writeheader()
        writer.writerows(report_data)
    print(f"Ingestion report saved to {CSV_FILE}")
    return CSV_FILE

def main():
    print(f"--- Starting Weekly Update: {datetime.datetime.now()} ---")
    
    # 1. Run scraping scripts
    scraping_results = run_scraping_scripts()
    
    # 2. Run knowledge base creation
    kb_result = run_knowledge_base_creation()
    
    # 3. Generate ingestion report
    report_file = generate_ingestion_report()
    
    print("\n--- Update Complete ---")
    print("Scraping Results:", scraping_results)
    print("Knowledge Base Result:", kb_result)
    print(f"Ingestion Report: {report_file}")

if __name__ == "__main__":
    main()


